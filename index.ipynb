{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-27T15:32:50.810423Z",
     "start_time": "2024-10-27T15:32:50.804775Z"
    }
   },
   "source": [
    "#1.install and import dependencies\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:32:52.258256Z",
     "start_time": "2024-10-27T15:32:52.254935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#2.Using MP Holistic to get the keypoints\n",
    "mp_holistic = mp.solutions.holistic #Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils #Drawing utilities"
   ],
   "id": "427b78294046aea5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:32:53.837393Z",
     "start_time": "2024-10-27T15:32:53.828940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#functions declarations\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ],
   "id": "6b4809812bd05c42",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #Read feed\n",
    "        ret, frame  = cap.read()\n",
    "        \n",
    "        #Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(image, results)\n",
    "        #print(results)\n",
    "        \n",
    "        #draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        #Show to screen\n",
    "        cv2.imshow(\"OpenCV Feed\", image)\n",
    "        \n",
    "        #Break gracefully when q key is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break"
   ],
   "id": "95d6eece7d9be6e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "b952f8b8a2ef7cc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "draw_landmarks(frame, results)\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n"
   ],
   "id": "2adee2b2556e1029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:21:54.423397Z",
     "start_time": "2024-10-27T15:21:53.683369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#3. Extract the keypoint values\n",
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)\n",
    "#print(pose)"
   ],
   "id": "6a17a6b763b15cd8",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#3. Extract the keypoint values\u001B[39;00m\n\u001B[0;32m      2\u001B[0m pose \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43mresults\u001B[49m\u001B[38;5;241m.\u001B[39mpose_landmarks\u001B[38;5;241m.\u001B[39mlandmark:\n\u001B[0;32m      4\u001B[0m     test \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([res\u001B[38;5;241m.\u001B[39mx, res\u001B[38;5;241m.\u001B[39my, res\u001B[38;5;241m.\u001B[39mz, res\u001B[38;5;241m.\u001B[39mvisibility])\n\u001B[0;32m      5\u001B[0m     pose\u001B[38;5;241m.\u001B[39mappend(test)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'results' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:32:59.400191Z",
     "start_time": "2024-10-27T15:32:59.393651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    # 468*3 + 33*4 + 21*3 + 21*3 \n",
    "    return np.concatenate([pose, face, lh, rh])"
   ],
   "id": "96d391c212ca72f2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result_test  = extract_keypoints(results)\n",
    "#np.save('0',result_test)\n",
    "#np.load('0.npy')"
   ],
   "id": "6de5803ed90008fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:33:07.931557Z",
     "start_time": "2024-10-27T15:33:07.927161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4.Setup folders to store the keypoints\n",
    "#Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_DATA')#the parent folder's path will starts with MP_DATA\n",
    "\n",
    "#Actions that we try to detect\n",
    "actions = np.array(['hello','thanks','iloveyou','ryoki tenkai'])\n",
    "\n",
    "#Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "#Each of the video will have 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "#Folder start\n",
    "start_folder = 30"
   ],
   "id": "13170394db3f5b2f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:33:09.827078Z",
     "start_time": "2024-10-27T15:33:09.805578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for action in actions:\n",
    "    #dirmax  = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int))\n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            #make directory MP_DATA/hello/0\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            #if there's exception just ignore it and keep going\n",
    "            pass"
   ],
   "id": "cef19e0029825e01",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:38:32.287966Z",
     "start_time": "2024-10-27T15:33:12.225497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5.Collecting keypoints values for training and testing \n",
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    #new loop \n",
    "    #loop through actions\n",
    "    for action in actions:\n",
    "        #loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            #loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "                #Read feed\n",
    "                ret, frame  = cap.read()\n",
    "                \n",
    "                #Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                \n",
    "                #draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                #new apply wait logic to give breaks\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, \"Starting Collection\", (120, 200),cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, \"Collecting frames for {} Video Number {}\".format(action, sequence), (15, 12),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0, 255), 1, cv2.LINE_AA)\n",
    "                    #Show to screen\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, \"Collecting frames for {} Video Number {}\".format(action, sequence), (15, 12),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0, 255), 1, cv2.LINE_AA) \n",
    "                    \n",
    "                cv2.imshow(\"Sign Language Recognition\", image)\n",
    "                #new export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                #Break gracefully when q key is pressed\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "    "
   ],
   "id": "85bb8ae029b5d712",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Workshop 2 Project\\action_recognition_model\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:01.662994Z",
     "start_time": "2024-10-27T15:39:01.638733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "79a8d0e42e1df350",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:18.247127Z",
     "start_time": "2024-10-27T15:39:17.532241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#6. Preprocess data and create labels and features\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "id": "f54e76ce2a557c6b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:20.606236Z",
     "start_time": "2024-10-27T15:39:20.597351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "label_map"
   ],
   "id": "46f1a34bf63d4fea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.str_('hello'): 0,\n",
       " np.str_('thanks'): 1,\n",
       " np.str_('iloveyou'): 2,\n",
       " np.str_('ryoki tenkai'): 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:32.800391Z",
     "start_time": "2024-10-27T15:39:31.908028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 30 sequences = 30 videos recorded , labels = y labels\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH,action))).astype(int):\n",
    "        #to store each of the video\n",
    "        window = []\n",
    "        #to append each of the frame together to form a complete video\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ],
   "id": "7aa46c1a190f24e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:34.157107Z",
     "start_time": "2024-10-27T15:39:34.147822Z"
    }
   },
   "cell_type": "code",
   "source": "np.array(labels).shape",
   "id": "9f20343b455d2415",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:36.321321Z",
     "start_time": "2024-10-27T15:39:36.294682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = np.array(sequences)\n",
    "x.shape"
   ],
   "id": "50a8f4abc9410d8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30, 1662)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:39.134745Z",
     "start_time": "2024-10-27T15:39:39.130955Z"
    }
   },
   "cell_type": "code",
   "source": "y = to_categorical(labels).astype(int)",
   "id": "84b92dc22d843363",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:41.584374Z",
     "start_time": "2024-10-27T15:39:41.553040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.05)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ],
   "id": "b4ef18040e243f72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114, 30, 1662) (6, 30, 1662) (114, 4) (6, 4)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:44.079944Z",
     "start_time": "2024-10-27T15:39:44.058436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#7. Build and train LSTM Neural Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Input\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ],
   "id": "6f2dbcfe68672bbd",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:48.291826Z",
     "start_time": "2024-10-27T15:39:48.288441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#to store the training logs, can view the progress using cmd: tensorboard --logdir=.\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ],
   "id": "7d0c6efc4d003dfd",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:39:51.097830Z",
     "start_time": "2024-10-27T15:39:50.858419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "#have to return the sequences so that the next layer can use them\n",
    "# Specify the input using Input layer\n",
    "model.add(Input(shape=(30, 1662)))\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "#softmax means the max value would be 1 and 1 would be allocated to each of the different actions as probability\n",
    "model.add(Dense(actions.shape[0],activation='softmax'))\n",
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])"
   ],
   "id": "508d2d04acfaaf00",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:44:20.823122Z",
     "start_time": "2024-10-27T15:44:11.365657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#train the model **this will takes time\n",
    "model.fit(x_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ],
   "id": "8a68002acef610f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 2.0422e-04\n",
      "Epoch 2/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - categorical_accuracy: 1.0000 - loss: 2.4106e-04\n",
      "Epoch 3/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - categorical_accuracy: 1.0000 - loss: 2.1135e-04\n",
      "Epoch 4/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - categorical_accuracy: 1.0000 - loss: 2.3398e-04\n",
      "Epoch 5/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - categorical_accuracy: 1.0000 - loss: 2.1604e-04\n",
      "Epoch 6/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 2.3660e-04\n",
      "Epoch 7/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - categorical_accuracy: 1.0000 - loss: 1.8813e-04\n",
      "Epoch 8/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step - categorical_accuracy: 1.0000 - loss: 1.8223e-04\n",
      "Epoch 9/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - categorical_accuracy: 1.0000 - loss: 1.8867e-04\n",
      "Epoch 10/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - categorical_accuracy: 1.0000 - loss: 1.7027e-04\n",
      "Epoch 11/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - categorical_accuracy: 1.0000 - loss: 1.7411e-04\n",
      "Epoch 12/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.4016e-04\n",
      "Epoch 13/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step - categorical_accuracy: 1.0000 - loss: 1.8348e-04\n",
      "Epoch 14/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - categorical_accuracy: 1.0000 - loss: 1.4432e-04\n",
      "Epoch 15/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.5723e-04\n",
      "Epoch 16/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - categorical_accuracy: 1.0000 - loss: 1.5104e-04\n",
      "Epoch 17/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.5948e-04\n",
      "Epoch 18/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - categorical_accuracy: 1.0000 - loss: 1.5595e-04\n",
      "Epoch 19/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - categorical_accuracy: 1.0000 - loss: 1.5555e-04\n",
      "Epoch 20/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - categorical_accuracy: 1.0000 - loss: 1.3523e-04\n",
      "Epoch 21/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.6679e-04\n",
      "Epoch 22/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - categorical_accuracy: 1.0000 - loss: 1.9497e-04\n",
      "Epoch 23/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.2944e-04\n",
      "Epoch 24/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.3990e-04\n",
      "Epoch 25/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - categorical_accuracy: 1.0000 - loss: 1.5651e-04\n",
      "Epoch 26/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.3541e-04\n",
      "Epoch 27/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.5239e-04\n",
      "Epoch 28/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step - categorical_accuracy: 1.0000 - loss: 1.1347e-04\n",
      "Epoch 29/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step - categorical_accuracy: 1.0000 - loss: 1.3288e-04\n",
      "Epoch 30/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - categorical_accuracy: 1.0000 - loss: 1.4400e-04\n",
      "Epoch 31/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - categorical_accuracy: 1.0000 - loss: 1.4472e-04\n",
      "Epoch 32/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - categorical_accuracy: 1.0000 - loss: 1.2435e-04\n",
      "Epoch 33/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - categorical_accuracy: 1.0000 - loss: 1.4821e-04\n",
      "Epoch 34/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.0343e-04\n",
      "Epoch 35/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - categorical_accuracy: 1.0000 - loss: 1.0813e-04\n",
      "Epoch 36/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.0094e-04\n",
      "Epoch 37/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - categorical_accuracy: 1.0000 - loss: 9.2136e-05\n",
      "Epoch 38/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - categorical_accuracy: 1.0000 - loss: 1.0146e-04\n",
      "Epoch 39/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - categorical_accuracy: 1.0000 - loss: 1.0170e-04\n",
      "Epoch 40/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - categorical_accuracy: 1.0000 - loss: 1.1429e-04\n",
      "Epoch 41/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step - categorical_accuracy: 1.0000 - loss: 8.9818e-05\n",
      "Epoch 42/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step - categorical_accuracy: 1.0000 - loss: 9.7817e-05\n",
      "Epoch 43/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step - categorical_accuracy: 1.0000 - loss: 7.9216e-05\n",
      "Epoch 44/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - categorical_accuracy: 1.0000 - loss: 1.1198e-04\n",
      "Epoch 45/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step - categorical_accuracy: 1.0000 - loss: 1.0630e-04\n",
      "Epoch 46/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - categorical_accuracy: 1.0000 - loss: 1.0099e-04\n",
      "Epoch 47/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - categorical_accuracy: 1.0000 - loss: 1.0088e-04\n",
      "Epoch 48/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - categorical_accuracy: 1.0000 - loss: 9.8823e-05\n",
      "Epoch 49/2000\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - categorical_accuracy: 1.0000 - loss: 1.0217e-04\n",
      "Epoch 50/2000\n",
      "\u001B[1m1/4\u001B[0m \u001B[32m━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - categorical_accuracy: 1.0000 - loss: 1.5714e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#train the model **this will takes time\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mtb_callback\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Workshop 2 Project\\action_recognition_model\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\Workshop 2 Project\\action_recognition_model\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:321\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[0;32m    319\u001B[0m callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m    320\u001B[0m logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(iterator)\n\u001B[1;32m--> 321\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Workshop 2 Project\\action_recognition_model\\.venv\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:174\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    172\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_batch_end(batch, logs)\n\u001B[1;32m--> 174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_async_train:\n\u001B[0;32m    176\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_async_dispatch(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_train_batch_end, batch, logs)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:44:24.392861Z",
     "start_time": "2024-10-27T15:44:24.356855Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "4d605169c0c15514",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │       \u001B[38;5;34m442,112\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m128\u001B[0m)        │        \u001B[38;5;34m98,816\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │        \u001B[38;5;34m49,408\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m4,160\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │         \u001B[38;5;34m2,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4\u001B[0m)              │           \u001B[38;5;34m132\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">442,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,790,126\u001B[0m (6.83 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,790,126</span> (6.83 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m596,708\u001B[0m (2.28 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">596,708</span> (2.28 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m1,193,418\u001B[0m (4.55 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,193,418</span> (4.55 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "210dd3a28a8fdd84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:44:41.437219Z",
     "start_time": "2024-10-27T15:44:40.858862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#8.Make predictions\n",
    "res = model.predict(x_test)\n",
    "actions[np.argmax(res[4])]"
   ],
   "id": "50c8321128e4c68f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 534ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.str_('hello')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:44:54.064371Z",
     "start_time": "2024-10-27T15:44:54.057326Z"
    }
   },
   "cell_type": "code",
   "source": "actions[np.argmax(y_test[3])]",
   "id": "c758ab401c96990f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('ryoki tenkai')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:44:59.611727Z",
     "start_time": "2024-10-27T15:44:59.515695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#9.Save the model \n",
    "model.save(\"recognition_model_v2.keras\")"
   ],
   "id": "10a930b21174ee58",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:27:46.437911Z",
     "start_time": "2024-10-27T08:27:46.434568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#delete the model\n",
    "del model"
   ],
   "id": "47a2447d43431ed8",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:37:09.695210Z",
     "start_time": "2024-10-27T08:37:09.604153Z"
    }
   },
   "cell_type": "code",
   "source": "model.load_weights(\"recognition_model_v1.keras\")",
   "id": "1bc5133a30861683",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:45:08.196829Z",
     "start_time": "2024-10-27T15:45:08.193828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#10. Evaluation using confusion matrix and accuracy\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ],
   "id": "d930dfd3ad6fb7e7",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:45:09.317575Z",
     "start_time": "2024-10-27T15:45:09.251722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "yhat = model.predict(x_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ],
   "id": "15750b52c895ae1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:45:13.488158Z",
     "start_time": "2024-10-27T15:45:13.473052Z"
    }
   },
   "cell_type": "code",
   "source": "multilabel_confusion_matrix(ytrue, yhat)",
   "id": "4eba39fbe9af10dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4, 0],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[4, 0],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[5, 1],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[4, 0],\n",
       "        [1, 1]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:45:17.149654Z",
     "start_time": "2024-10-27T15:45:17.143089Z"
    }
   },
   "cell_type": "code",
   "source": "accuracy_score(ytrue, yhat)",
   "id": "7ac64bb34952fc6c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:45:16.321606Z",
     "start_time": "2024-10-27T15:45:16.318236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#11.Test in real time\n",
    "from scipy import stats"
   ],
   "id": "780b6c3fa09dce9e",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:46:20.581836Z",
     "start_time": "2024-10-27T15:46:20.577029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "#function to draw the probability bars on each of the actions\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 60+num*40), (int(prob*100), 90 +num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    return output_frame"
   ],
   "id": "cbd320d8c84c17ff",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:46:21.614523Z",
     "start_time": "2024-10-27T15:46:21.575603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#plot the output figure\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ],
   "id": "671901c5ca24a7fd",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#plot the output figure\u001B[39;00m\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m9\u001B[39m,\u001B[38;5;241m9\u001B[39m))\n\u001B[1;32m----> 3\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(\u001B[43mprob_viz\u001B[49m\u001B[43m(\u001B[49m\u001B[43mres\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolors\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[1;32mIn[38], line 6\u001B[0m, in \u001B[0;36mprob_viz\u001B[1;34m(res, actions, input_frame, colors)\u001B[0m\n\u001B[0;32m      4\u001B[0m output_frame \u001B[38;5;241m=\u001B[39m input_frame\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m num, prob \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(res):\n\u001B[1;32m----> 6\u001B[0m     cv2\u001B[38;5;241m.\u001B[39mrectangle(output_frame, (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m60\u001B[39m\u001B[38;5;241m+\u001B[39mnum\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m40\u001B[39m), (\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprob\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;241m90\u001B[39m \u001B[38;5;241m+\u001B[39mnum\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m40\u001B[39m), colors[num], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      7\u001B[0m     cv2\u001B[38;5;241m.\u001B[39mputText(output_frame, actions[num], (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m85\u001B[39m\u001B[38;5;241m+\u001B[39mnum\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m40\u001B[39m), cv2\u001B[38;5;241m.\u001B[39mFONT_HERSHEY_SIMPLEX, \u001B[38;5;241m1\u001B[39m, (\u001B[38;5;241m255\u001B[39m,\u001B[38;5;241m255\u001B[39m,\u001B[38;5;241m255\u001B[39m), \u001B[38;5;241m2\u001B[39m, cv2\u001B[38;5;241m.\u001B[39mLINE_AA)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output_frame\n",
      "\u001B[1;31mTypeError\u001B[0m: only length-1 arrays can be converted to Python scalars"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-27T15:47:49.921721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#testing the model\n",
    "#new variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #Read feed\n",
    "        ret, frame  = cap.read()\n",
    "        \n",
    "        #Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(image, results)\n",
    "        #print(results)\n",
    "        \n",
    "        #draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        #prediction logic \n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        #get the latest 30 keypoints\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        #make prediction every 30 frame\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(\"Predicted Action: \" + actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            #Viz logic: to check whether the last prediction is the same as the current one\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "                if res[np.argmax(res)] > threshold:\n",
    "                    if len(sentence) > 0:\n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "            if len(sentence) > 1:\n",
    "                sentence = sentence[-1:]\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        #Show to screen\n",
    "        cv2.imshow(\"Sign Language Recognition\", image)\n",
    "        \n",
    "        #Break gracefully when q key is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "3d609a20ff3b3489",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
