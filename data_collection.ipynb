{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:19:53.183889Z",
     "start_time": "2024-12-01T14:19:48.353501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#1. Import the libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, Conv1D, MaxPooling1D, GRU, TimeDistributed, Flatten ,Conv2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import mediapipe as mp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "id": "c1fd316d9a46cb5b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:19:53.191351Z",
     "start_time": "2024-12-01T14:19:53.187909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#2. Declare and initiliaze paths and constants\n",
    "# DATA_PATH = 'DATA'  # Path to save videos and keypoints\n",
    "DATA_PATH = 'NEW_DATA'  # Path to save new videos and keypoints for adding new data\n",
    "\n",
    "# Accumulated actions\n",
    "# ACTIONS = np.array(['Mana'])\n",
    "\n",
    "# New actions to be added to the dataset\n",
    "ACTIONS = np.array(['Hi'])\n",
    "\n",
    "NO_SEQUENCES = 90  # Number of videos per action\n",
    "SEQUENCE_LENGTH = 30  # Frames per video"
   ],
   "id": "22ea4846eb93a706",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:19:55.270654Z",
     "start_time": "2024-12-01T14:19:55.259096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#3. Declare the functions for collecting data\n",
    "# Mediapipe setup\n",
    "mp_holistic = mp.solutions.holistic  # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils  # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    return image, results\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([pose, lh, rh])\n",
    "\n",
    "def extract_keypoints_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    keypoints = []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            _, results = mediapipe_detection(frame, holistic)\n",
    "            keypoint_frame = extract_keypoints(results)\n",
    "            keypoints.append(keypoint_frame)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(keypoints)\n",
    "\n",
    "def process_videos_to_keypoints(start_sequence=0, end_sequence=None):\n",
    "    \"\"\"\n",
    "    Process videos to extract keypoints and save as .npy files,\n",
    "    keeping .npy indices aligned with video indices.\n",
    "    \"\"\"\n",
    "    for action in ACTIONS:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        \n",
    "        # Get all video files in the action directory\n",
    "        video_files = [f for f in os.listdir(action_path) if f.endswith(\".mp4\")]\n",
    "        \n",
    "        # Sort files numerically based on the numeric part of their names\n",
    "        video_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x)) or 0))\n",
    "        \n",
    "        # If end_index is None, process all files from start_index onwards\n",
    "        if end_sequence is None:\n",
    "            end_sequence = len(video_files)\n",
    "        \n",
    "        for video_file in video_files[start_sequence:end_sequence]:\n",
    "            # Extract the numeric index from the video filename\n",
    "            video_index = int(''.join(filter(str.isdigit, os.path.splitext(video_file)[0])) or 0)\n",
    "            \n",
    "            video_path = os.path.join(action_path, video_file)\n",
    "            keypoints = extract_keypoints_from_video(video_path)\n",
    "            \n",
    "            # Save keypoints as .npy, ensuring consistent zero-based naming\n",
    "            npy_path = os.path.join(action_path, f\"{video_index}.npy\")\n",
    "            np.save(npy_path, keypoints)\n",
    "            print(f\"Processed and saved keypoints for Action({action}) {video_file} as {video_index}.npy\")\n",
    "\n",
    "def record_videos():\n",
    "    # default will record 30 video\n",
    "    # start_sequence = 0\n",
    "    # last_sequence = 30\n",
    "    start_sequence = 0\n",
    "    last_sequence = 30\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "    for action in ACTIONS:\n",
    "        os.makedirs(os.path.join(DATA_PATH, action), exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_rate = 30  # Frames per second\n",
    "    video_duration = 3  # Duration in seconds\n",
    "    total_frames = frame_rate * video_duration\n",
    "    \n",
    "    for action in ACTIONS:\n",
    "        print(f\"Recording for action: {action}\")\n",
    "        for sequence in range(start_sequence, last_sequence):\n",
    "            video_path = os.path.join(DATA_PATH, action, f\"{sequence}.mp4\")\n",
    "            out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (640, 480))\n",
    "\n",
    "            for frame_num in range(total_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Failed to capture frame.\")\n",
    "                    break\n",
    "\n",
    "                # Display recording information\n",
    "                cv2.rectangle(frame, (0,0), (640, 60), (255, 255, 255), -1)\n",
    "                cv2.putText(frame, f\"Recording {action}: Video {sequence+1}/{last_sequence}\", \n",
    "                            (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (14, 14, 14), 2)\n",
    "                cv2.imshow(\"Recording\", frame)\n",
    "\n",
    "                out.write(frame)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    print(\"Recording interrupted.\")\n",
    "                    break\n",
    "\n",
    "            out.release()\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "id": "ed432f31f36f403",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:15:17.451114Z",
     "start_time": "2024-12-01T14:12:41.949154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#4. Run the function to start recording the data into mp4\n",
    "record_videos()"
   ],
   "id": "7501867296ad06d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for action: Hi\n",
      "Recording interrupted.\n",
      "Recording interrupted.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#4. Run the function to start recording the data into mp4\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mrecord_videos\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 87\u001B[0m, in \u001B[0;36mrecord_videos\u001B[1;34m()\u001B[0m\n\u001B[0;32m     84\u001B[0m out \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mVideoWriter(video_path, cv2\u001B[38;5;241m.\u001B[39mVideoWriter_fourcc(\u001B[38;5;241m*\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmp4v\u001B[39m\u001B[38;5;124m'\u001B[39m), frame_rate, (\u001B[38;5;241m640\u001B[39m, \u001B[38;5;241m480\u001B[39m))\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m frame_num \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(total_frames):\n\u001B[1;32m---> 87\u001B[0m     ret, frame \u001B[38;5;241m=\u001B[39m \u001B[43mcap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[0;32m     89\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to capture frame.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:21:16.135023Z",
     "start_time": "2024-12-01T14:19:59.490052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#5. Convert mp4 to npy(numpy array)\n",
    "\n",
    "# Process sequences from index (start_sequence) to (end_sequence-not inclusive) for all actions\n",
    "process_videos_to_keypoints(start_sequence=0, end_sequence=90)"
   ],
   "id": "1933b2f67f826b8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Workshop 2 Project\\action_recognition_model\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved keypoints for Action(Hi) 0.mp4 as 0.npy\n",
      "Processed and saved keypoints for Action(Hi) 1.mp4 as 1.npy\n",
      "Processed and saved keypoints for Action(Hi) 2.mp4 as 2.npy\n",
      "Processed and saved keypoints for Action(Hi) 3.mp4 as 3.npy\n",
      "Processed and saved keypoints for Action(Hi) 4.mp4 as 4.npy\n",
      "Processed and saved keypoints for Action(Hi) 5.mp4 as 5.npy\n",
      "Processed and saved keypoints for Action(Hi) 6.mp4 as 6.npy\n",
      "Processed and saved keypoints for Action(Hi) 7.mp4 as 7.npy\n",
      "Processed and saved keypoints for Action(Hi) 8.mp4 as 8.npy\n",
      "Processed and saved keypoints for Action(Hi) 9.mp4 as 9.npy\n",
      "Processed and saved keypoints for Action(Hi) 10.mp4 as 10.npy\n",
      "Processed and saved keypoints for Action(Hi) 11.mp4 as 11.npy\n",
      "Processed and saved keypoints for Action(Hi) 12.mp4 as 12.npy\n",
      "Processed and saved keypoints for Action(Hi) 13.mp4 as 13.npy\n",
      "Processed and saved keypoints for Action(Hi) 14.mp4 as 14.npy\n",
      "Processed and saved keypoints for Action(Hi) 15.mp4 as 15.npy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#5. Convert mp4 to npy(numpy array)\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Process sequences from index (start_sequence) to (end_sequence-not inclusive) for all actions\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[43mprocess_videos_to_keypoints\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_sequence\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_sequence\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m90\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[4], line 58\u001B[0m, in \u001B[0;36mprocess_videos_to_keypoints\u001B[1;34m(start_sequence, end_sequence)\u001B[0m\n\u001B[0;32m     55\u001B[0m video_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28mstr\u001B[39m\u001B[38;5;241m.\u001B[39misdigit, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplitext(video_file)[\u001B[38;5;241m0\u001B[39m])) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     57\u001B[0m video_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(action_path, video_file)\n\u001B[1;32m---> 58\u001B[0m keypoints \u001B[38;5;241m=\u001B[39m \u001B[43mextract_keypoints_from_video\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;66;03m# Save keypoints as .npy, ensuring consistent zero-based naming\u001B[39;00m\n\u001B[0;32m     61\u001B[0m npy_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(action_path, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[4], line 28\u001B[0m, in \u001B[0;36mextract_keypoints_from_video\u001B[1;34m(video_path)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m _, results \u001B[38;5;241m=\u001B[39m \u001B[43mmediapipe_detection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mholistic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m keypoint_frame \u001B[38;5;241m=\u001B[39m extract_keypoints(results)\n\u001B[0;32m     30\u001B[0m keypoints\u001B[38;5;241m.\u001B[39mappend(keypoint_frame)\n",
      "Cell \u001B[1;32mIn[4], line 9\u001B[0m, in \u001B[0;36mmediapipe_detection\u001B[1;34m(image, model)\u001B[0m\n\u001B[0;32m      7\u001B[0m image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(image, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2RGB)\n\u001B[0;32m      8\u001B[0m image\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mwriteable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m image\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mwriteable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image, results\n",
      "File \u001B[1;32mD:\\Workshop 2 Project\\action_recognition_model\\.venv\\Lib\\site-packages\\mediapipe\\python\\solutions\\holistic.py:160\u001B[0m, in \u001B[0;36mHolistic.process\u001B[1;34m(self, image)\u001B[0m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NamedTuple:\n\u001B[0;32m    137\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001B[39;00m\n\u001B[0;32m    138\u001B[0m \n\u001B[0;32m    139\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03m         \"enable_segmentation\" is set to true.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 160\u001B[0m   results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    161\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m results\u001B[38;5;241m.\u001B[39mpose_landmarks:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m landmark \u001B[38;5;129;01min\u001B[39;00m results\u001B[38;5;241m.\u001B[39mpose_landmarks\u001B[38;5;241m.\u001B[39mlandmark:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Workshop 2 Project\\action_recognition_model\\.venv\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001B[0m, in \u001B[0;36mSolutionBase.process\u001B[1;34m(self, input_data)\u001B[0m\n\u001B[0;32m    334\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39madd_packet_to_input_stream(\n\u001B[0;32m    336\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream_name,\n\u001B[0;32m    337\u001B[0m         packet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_packet(input_stream_type,\n\u001B[0;32m    338\u001B[0m                                  data)\u001B[38;5;241m.\u001B[39mat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_simulated_timestamp))\n\u001B[1;32m--> 340\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait_until_idle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001B[39;00m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;66;03m# output stream names.\u001B[39;00m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_stream_type_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:22:00.476299Z",
     "start_time": "2024-12-01T14:21:55.712412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#6. Visualize the keypoints for a specific data recording\n",
    "\n",
    "# Define Mediapipe connections (assumes 33 keypoints)\n",
    "# Updated connections to include only pose, left hand, and right hand\n",
    "POSE_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 7),  # Upper body\n",
    "    (0, 4), (4, 5), (5, 6), (6, 8),  # Upper body\n",
    "    (9, 10),  # Neck\n",
    "    (11, 12), (11, 13), (13, 15), (15, 17),  # Left side\n",
    "    (12, 14), (14, 16), (16, 18)  # Right side\n",
    "]\n",
    "\n",
    "HAND_CONNECTIONS = [\n",
    "    # Connections for fingers (same for both hands)\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),  # Thumb\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),  # Index finger\n",
    "    (5, 9), (9, 10), (10, 11), (11, 12),  # Middle finger\n",
    "    (9, 13), (13, 14), (14, 15), (15, 16),  # Ring finger\n",
    "    (13, 17), (17, 18), (18, 19), (19, 20)  # Pinky\n",
    "]\n",
    "\n",
    "\n",
    "def visualize_extracted_keypoints(keypoints, action_name, frame_idx):\n",
    "    \"\"\"\n",
    "    Visualize extracted keypoints (pose, left hand, right hand).\n",
    "\n",
    "    :param keypoints: Numpy array of shape (num_keypoints,)\n",
    "                      Includes pose (33*4), left hand (21*3), right hand (21*3).\n",
    "    :param action_name: String, action name\n",
    "    :param frame_idx: Integer, current frame index for labeling\n",
    "    \"\"\"\n",
    "    canvas = np.ones((480, 640, 3), dtype=np.uint8) * 255  # Blank white canvas\n",
    "\n",
    "    # Extract pose, left hand, and right hand keypoints\n",
    "    pose = keypoints[:33 * 4].reshape((33, 4))[:, :2] * [640, 480]  # Normalize to image size\n",
    "    lh = keypoints[33 * 4:33 * 4 + 21 * 3].reshape((21, 3))[:, :2] * [640, 480]\n",
    "    rh = keypoints[33 * 4 + 21 * 3:].reshape((21, 3))[:, :2] * [640, 480]\n",
    "\n",
    "    # Draw pose connections\n",
    "    for start, end in POSE_CONNECTIONS:\n",
    "        x1, y1 = pose[start]\n",
    "        x2, y2 = pose[end]\n",
    "        cv2.line(canvas, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "    for x, y in pose:\n",
    "        cv2.circle(canvas, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Draw left hand connections\n",
    "    for start, end in HAND_CONNECTIONS:\n",
    "        if start < len(lh) and end < len(lh):\n",
    "            x1, y1 = lh[start]\n",
    "            x2, y2 = lh[end]\n",
    "            cv2.line(canvas, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "    for x, y in lh:\n",
    "        cv2.circle(canvas, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Draw right hand connections\n",
    "    for start, end in HAND_CONNECTIONS:\n",
    "        if start < len(rh) and end < len(rh):\n",
    "            x1, y1 = rh[start]\n",
    "            x2, y2 = rh[end]\n",
    "            cv2.line(canvas, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "    for x, y in rh:\n",
    "        cv2.circle(canvas, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "    # Add action label and frame info\n",
    "    cv2.putText(canvas, f\"Frame: {frame_idx}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "    # Display the canvas\n",
    "    cv2.imshow(\"Extracted Keypoint Visualization\", canvas)\n",
    "    cv2.waitKey(50)  # Adjust delay between frames\n",
    "\n",
    "# Load extracted keypoints\n",
    "keypoints_data = np.load(f\"{DATA_PATH}/Hi/10.npy\")  # Shape (NUM_SEQUENCES, SEQUENCE_LENGTH, num_keypoints)\n",
    "SEQUENCE_LENGTH = 30  # Adjust based on your actual data\n",
    "NUM_KEYPOINTS = 258\n",
    "reshaped_data = keypoints_data.reshape(-1, SEQUENCE_LENGTH, NUM_KEYPOINTS)\n",
    "\n",
    "for sequence_idx, sequence in enumerate(reshaped_data):\n",
    "    for frame_idx, keypoints in enumerate(sequence):\n",
    "        # Reshape and normalize keypoints for visualization\n",
    "        pose = keypoints[:33 * 4].reshape((33, 4))[:, :2] * [640, 480]  # Pose keypoints\n",
    "        lh = keypoints[33 * 4:33 * 4 + 21 * 3].reshape((21, 3))[:, :2] * [640, 480]  # Left hand\n",
    "        rh = keypoints[33 * 4 + 21 * 3:].reshape((21, 3))[:, :2] * [640, 480]  # Right hand\n",
    "\n",
    "        visualize_extracted_keypoints(keypoints, f\"Action_{sequence_idx}\", frame_idx)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "b74a24a0e542f025",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
